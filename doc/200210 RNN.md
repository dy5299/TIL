# RNN

Recurrent Neural Network

일반 신경망(FFNets) : 개별 데이터를 독립적으로 학습

순환 신경망(RNN) : 시계열 데이터 학습에 적합

은닉층 : y = tanh(  W_x*x_t + W_h*_(t-1)+b  )    #기존에 비해 t-1 term이 추가된 것 뿐.

출력층: y_t = f(  W_y*h_t+b  )

character-level language model

~

vanilla neural networks : 위를 기반으로 해서.

입력이 하나 들어가면 출력이 여러개 나오는.

Machine translation: seq of words -? seq of words

RNN들을 네트웍을 어떻게 설계하냐에 따라 응용분야가 다양하다.



## LSTM

BUT 실제로 RNN으로 구현하는 경우는 거의 없다.

RNN의 문제는, 층이 깊어지면, 문장이 길어지면 학습이 잘 안 된다.

예측하는데 필요한 문맥이 가까이 있고, 많지 않다면 RNN은 이를 학습 가능하다.

정보를 필요로 하는 시점과, 필요한 정보가 멀리 떨어져있다면 잘 예측할 수 없다 (=Long-term dependency)

RNN의 히든 state에 cell-state를 추가 => LSTM(Long Short-Term Memory)

state가 오래 경과하더라도 그라디언트가 비교적 전파가 잘 되도록 한다.



- 순환 신경망의 학습 유형

RNN은 분석 목적에 따라 아래와 같이 여러 가지 유형으로 학습시킬 수 있다.

RNN의 유형은 입력값에 대응하는 출력값의 개수에 따라 아래 4가지 유형으로 분류할 수 있다.



Classification 목적을 위해서는 many-to-one 유형을 사용할 수 있고, 시계열 분석은 many-to-one이나 many-to-many 유형을 사용할 수 있다.

참고로 이미지를 말로 설명하는 image captioning 기능은 one-to-many 유형으로 학습할 수 있고 (CNN으로 이미지를 인식하고 RNN으로 문장을 생성함), 자동번역기 (machine translation)는 (아래 3번째 유형의) many-to-many 유형으로 학습할 수 있다.