# RNN

(Recurrent Neural Networks)

일반 신경망(FFNets) : 개별 데이터를 독립적으로 학습

순환 신경망(RNN) : 시계열 데이터 학습에 적합

- 은닉층

$$
h_t = tanh(  W_xx_t + W_hh_{t-1}+b  )
$$
​    기존의 신경망에 비해 t-1 term이 추가된 것 뿐.

- 출력층

$$
y_t = f(  W_yh_t+b  )
$$
단, f는 비선형 활성화 함수 중 하나이다.

- dimension

배치 크기가 1이고, d와 Dh 두 값 모두를 4로 가정하였을 때, RNN의 은닉층 연산을 그림으로 표현하면 아래와 같습니다.
$$
x_t : (d×1)
$$
$$
W_x : (D_h×d)
$$
$$
W_h : (D_h×D_h)
$$
$$
h_{t-1} : (D_h×1)
$$
$$
b : (D_h×1)
$$


![200210_rnn_dim](images/200210_rnn_dim.png)

이 때 ht를 계산하는 과정 중 활성화 함수로 주로 hyperbolic tangent 함수가 사용된다.

각각의 가중치 Wx, Wh, Wy 들은 모든 시점에서 동일하게 공유한다. 만약 은닉층이 2개 이상일 경우 은닉층 2개의 가중치는 서로 다르다.

출력층은 결과값인 yt를 계산하기 위한 활성화 함수로는 상황에 따라 다를텐데, 예를 들어서 이진 분류를 해야하는 경우라면 시그모이드 함수를 사용할 수 있고 다양한 카테고리 중에서 선택해야하는 문제라면 소프트맥스 함수를 사용할 것이다.



character-level language model

~

vanilla neural networks : 위를 기반으로 해서.

입력이 하나 들어가면 출력이 여러개 나오는.

Machine translation: seq of words -? seq of words

RNN들을 네트웍을 어떻게 설계하냐에 따라 응용분야가 다양하다.



# LSTM

(Long Short-Term Memory models)

RNN의 일종

BUT 실제로 RNN으로 구현하는 경우는 거의 없다.

RNN의 문제는, 층이 깊어지면, 문장이 길어지면 학습이 잘 안 된다.

예측하는데 필요한 문맥이 가까이 있고, 많지 않다면 RNN은 이를 학습 가능하다.

정보를 필요로 하는 시점과, 필요한 정보가 멀리 떨어져있다면 잘 예측할 수 없다 (=Long-term dependency)

RNN의 히든 state에 cell-state를 추가 => LSTM

state가 오래 경과하더라도 그라디언트가 비교적 전파가 잘 되도록 한다.



- 순환 신경망의 학습 유형

RNN은 분석 목적에 따라 아래와 같이 여러 가지 유형으로 학습시킬 수 있다.

RNN의 유형은 입력값에 대응하는 출력값의 개수에 따라 아래 4가지 유형으로 분류할 수 있다.



Classification 목적을 위해서는 many-to-one 유형을 사용할 수 있고, 시계열 분석은 many-to-one이나 many-to-many 유형을 사용할 수 있다.

참고로 이미지를 말로 설명하는 image captioning 기능은 one-to-many 유형으로 학습할 수 있고 (CNN으로 이미지를 인식하고 RNN으로 문장을 생성함), 자동번역기 (machine translation)는 (아래 3번째 유형의) many-to-many 유형으로 학습할 수 있다.

## Practice

시계열 데이터는 y를 학습한다기보다는 sequence data를 학습하는 것이다.