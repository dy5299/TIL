# DNN

## 커리큘럼

8일 동안 안내된 커리큘럼은 다음과 같은데

전이학습 (객체인식)

객체검출

CNN ReLU -----------> 먼저 할 게. Deep learing, CNN 복잡한 구조..

영상 감정 학습



다음 챗봇 (나중)



실습: Tensorflow or Kerras 중 Kerras로 할 거야. Tensorflow는  사소한 부분에서도 에러 많아서

실습환경? AWS 계정 공급될 거야



## 중간 체크

1. 기계학습과 딥러닝의 차이점은?

   AI ⊃ 기계학습 ⊃ 딥러닝.

   신경망 -> 가중치.

   기계학습 : 주어진 데이터를 이용하여 값을 예측하는 모델링을 만드는 것

   보통은 직선을 찾으려 함, 쉬우니까

   수학적으로는 직선 방정식 문제: MLS :다층구조 불가.

   어떻게 잘 분류할 것인가?

   - SVM(Support Vector Merchine): 직선을 찾는 건 맞는데.. 알고리즘: 경계값 마지노선 라인의 중간선을 구하는 것. 두 값만 사용
   - 신경망: y=ax+b 의 a,b를 구하는데 반복적인 최적화를 통해 최적화된 a,b 찾기(이 알고리즘=경사하강법)

2. 경사하강법 그림으로 표현하면?

   경사하강법: 다층 네트워크 가능. 직선이 아니라 신경망으로 봄.

   깊이가 깊은 네트웤=딥러닝.

3. 가중치는 (생물학적인) 신경망의 어디에 해당하는가?

   시냅스의 신경전달물질(NTM) 양 = Weight

   W만 복사하면 학습할 필요 없다. W를 학습하는 것.

   ![200120_threshold](images/200120_threshold.PNG)
   b: 신경망에서는 bias (기하학에서는 y절편)

   대소관계 따라 0 or 1.

4. 어떻게 다층으로 복잡한 문제를 풀 수 있는가?

   b를 없앤 수식은 이렇게 표현하기도
   ![200120_threshold2](images/200120_threshold2.PNG)

5. 다층 네트웍을 학습시키기가 어려운 이유는?

6. y=wx+b 에서 b는 왜 필요한가?

   직선 모델링 시 원점을 지나야 한다는 건 큰 제약조건.

   batch normalization하면 w에 덜 의존적, 성능 향상.

7. 활성화는 비선형이어야 하는가?

8. sigmoid 함수의 장점은?

9. 딥러닝의 핵심 돌파구는?

   왜 층이 깊어질수록 성능이 나빠지는가?

   층마다 학습된 것/안 된 것이 있는데. 뒷층으로 갈 수록 학습이 안 돼

   학습이 안 된 것 = random. weight가 처음은 randomly잖아->결과도 안 나와

   (코드상) 뒤에 있는 것은 학습이 잘 돼, 앞으로 갈 수록 학습이 안 돼.

   신경망 핵심 알고리즘: MLP에서 역전파 알고리즘이라서.

   ​	목적값(target value)이 맨 뒤에 있음.

   ​	그 앞의 target value가 없는 거야. 학습 어떻게 시켜?

   ​	다층 구조에서 학습하는 방법 개발 -> 역전파

   ​	깊어질 수록 위의 weight value는 잘 안 변해, random value니까

   ​	-> 예측할 때 위의 값이 랜덤벨류니까 결과가 나쁨

10. 왜 cnn은 특징추출과 분류기결합으로 보는가?





python을 python답게 쓰는 방법

if문을 안 쓰는 것 - vertor... matrix 이용 -> gpu 활용해서 계산 능력 향상

모든 문제를 행렬로 변환하는게 편해.

numpy : 다차원 배열. 장점1: 고속으로 계산 가능 (몇백배) 장점2: 단일 data type만 들어가



NVIDIA - GPU v.s. CPU (유투브)

GPU가 없었다면 지금의 딥러닝 시대는 못 왔을 것

CPU는 color도 어렵

GPU는 80ms만에  가능

GPU는 단순 산술 연산만 할 수 있는 것들이 여러 개 모여있는 것 -> 단순반복 시 유리



## XOR

### tensorflow 2.0.0 설치

```python
conda create -n envname
conda activate envname
conda tensorflow version==2.0.0
conda uninstall tensorflow
```



### SGD (Stochastic Gradient Descent)

경사하강법: 방향의 문제. not 길이. 미분의 반대 방향으로.

확률적~: 방향 결정이 확률적이다

![200120_sgd](images/200120_sgd.png)

n = learning rate (step size) : 기울기 반대 방향으로 얼만큼 x를 움직일지

batch size: 한번에 네트워크에 넘겨주는 데이터의 수

​					gpu는 메모리가 작아. 큰 데이터는 못 넣음 -> batch size를 작게 넣어줌 (v.s. cpu는 무한대 가상메모리 given)

epochs: 학습 수. 다시 실행시키면 현재 시점에서 다시 시작해

​				전체 데이터를 한번 다 돌아서 학습한 1 cycle = 1 epoch

​				반복시키면 현재 시점에서 시작함



google)

1. w1에 대한 시작점을 선택합니다.

2. 시작점에서 손실 곡선의 기울기(Gradient)를 계산합니다.

   여기서 기울기는 편미분의 벡터로, 어느 방향이 더 정확한지 혹은 더 부정확한지를 알려줍니다.
   단일 가중치에 대한 손실의 기울기는 미분 값과 같습니다.

   손실함수 곡선의 다음 지점을 결정하기 위해
   경사하강법(Gradient Descent) 알고리즘은 단일 가중의 일부를 시작점에 더합니다.
   ( 어느 방향(+, -)으로 이동해야하는지를 결정함 )

   기울기의 보폭(Learning rate)을 통해 손실 곡선의 다음 지점으로 이동합니다.

3. 경사하강법은 위의 과정을 반복해 최소값에 점점 접근합니다.

   즉, 확률적 경사하강법은 데이터 세트에서 무작위로 균일하게 선택한 하나의 예를 의존하여
   각 단계의 예측 경사를 계산합니다.

   배치: 경사하강법에서 배치는 단일 반복에서 기울기를 계산하는 데 사용하는 예(data)의 총 개수입니다.

   Gradient Descent 에서의 배치는 전체 데이터 셋라고 가정합니다.

   하지만 대규모의 작업에서는...
   데이터 세트에서 예(data)를 무작위로 선택하면 (노이즈는 있겠지만) 훨씬 적은 데이터 세트로 중요한 평균값을 추정할 수 있습니다. 
   확률적 경사하강법(SGD)은 이 아이디어를 더욱 확장한 것으로서, 반복당 하나의 예(배치 크기 1)만을 사용합니다.

   '확률적(Stochastic)'이라는 용어는 각 배치를 포함하는 하나의 예가 무작위로 선택된다는 것을 의미합니다.

* 단점

  반복이 충분하면 SGD가 효과는 있지만 노이즈가 매우 심합니다.
  확률적 경사하강법의 여러 변형 함수의 최저점에 가까운 점을 찾을 가능성이 높지만 항상 보장되지는 않습니다.
  (최저점을 찾지는 못할 수 있음)

* 대안

  미니 배치 확률적 경사하강법(미니 배치 SGD)는 전체 배치 반복과 SGD 의 절충안입니다.

  미니 배치는 일반적으로 무작위로 선택한 10개에서 1,000개 사이의 예로 구성됩니다.
  미니 배치 SGD는 SGD의 노이즈를 줄이면서도 전체 배치보다는 더 효율적입니다.

### LambdaCallback, EarlyStopping

jupyter notebook에서 중간에 확인 가능

### LeakyReLU (activation ftn)

deep network의 문제를 해결 가능 (=> 딥러닝의 돌파구)

모든 계층이 학습이 거의 다 됨 -> 학습 속도 빠름

500번만에 99% accuracy 달성

sigmoid: 수렴하는 구조 / LeakyReLU: 발산하는 구조

sigmoid: 미분값 0이 아닌 구간이 매우 좁음 / LeakyReLU: 넓음

sigmoid: e함수 사용해서 굉장히 느림 / LeakyReLU: 계산이 매우 빠름

(200121)

# Advanced MLP

Advanced techniques for training neural networks

## Basic MLP model

```python
# reshaping X data: (n, 28, 28) => (n, 784)
# X set 2차원을 1차원으로
X_train = X_train.reshape((X_train.shape[0], -1))  #나머지 영역을 묶어 한차원으로
X_test = X_test.reshape((X_test.shape[0], -1))
```



```python
# use only 33% of training data to expedite the training process
X_train, _ , y_train, _ = train_test_split(X_train, y_train, test_size = 0.67, random_state = 7)
#자주 쓰는 함수라서. split하여 (1-p), p 순으로 리턴
```



```python
# converting y data into categorical (one-hot encoding)
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
# one-hot encoding: 1차원 벡터->2차원 벡터
# 0 -> 1 0 0  이 한 줄이 one binary class
# 1 -> 0 1 0  행렬로 구하면 빨라
# 2 -> 0 0 1
```



```python
model.add(Dense(50, input_shape = (784, )))
model.add(Activation('sigmoid'))
model.add(Dense(50))
model.add(Activation('sigmoid'))
model.add(Dense(50))
model.add(Activation('sigmoid'))
model.add(Dense(50))
model.add(Activation('sigmoid'))
model.add(Dense(10))
model.add(Activation('softmax'))
#binary 땐 sigmoid 썼지만
#multiple는 softmax 사용. binary와 똑같지만 여러번 처리
#softmax: sigmoid와의 차이점은 정규화해줌 (값의 합 1로 scaling)
#출력갯수 10개, class가 10개니까
```









## Weight Initialization

SGD에서 w 초기값?

#### Sigmoid에서 문제 - 

sigmoid 함수의 변수범위를 참고하여 -4~4로 초기화-> 대부분 출력값이 0, 1이 대부분이라. 학습이 안 됨

w값이 큰 게 문제다 -> N(0, 0.1) 정규분포로 초기화 -> sigmoid 분포

weight값은 +,-범위에 굉장히 작은 값이어야 한다.

#### Xavier Initialization

이전 노드와 다음 노드의 개수에 의존하여 표준편차 계산

#### ReLU에서 문제 - He Initialization

Xavier 방법이 비효율적이라서.

기본적으로 w값이 매우 작아야(sd작아야) 발산하는 경우가 없음









## Nonlinearity (Activation function)

## Optimizers

## Batch Normalization

## Dropout (Regularization)

## Model Ensemble